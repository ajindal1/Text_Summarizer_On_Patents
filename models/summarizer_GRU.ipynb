{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.1.0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "#print sys.executable\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import re\n",
    "import os\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "import time\n",
    "\n",
    "from tensorflow.python.layers.core import Dense\n",
    "from tensorflow.python.ops.rnn_cell_impl import _zero_state_tensors\n",
    "print('TensorFlow Version: {}' .format(tf.__version__))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"/home/abhishekjindal/nlp/data/\"\n",
    "reviews = pd.read_csv(os.path.join(data_dir,\"results-20180309-000248_8000_before_2010.csv\"), encoding = \"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8000, 4)\n",
      "Novel compounds for treatment of cancer and disorders associated with angiogenesis function\n"
     ]
    }
   ],
   "source": [
    "print(reviews.shape)\n",
    "print(reviews.title[0])#.head(1).values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "publication_date      0\n",
       "publication_number    0\n",
       "title                 0\n",
       "abstract              0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for any nulls values\n",
    "reviews.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patent #1 \n",
      "Novel compounds for treatment of cancer and disorders associated with angiogenesis function\n",
      "Novel compounds for treatment of cancer and disorders associated with angiogenesis function. Also disclosed are a method of preparing the compounds, pharmaceutical compositions and packaged products containing the compounds, a method of using these molecules to treat cancer (e.g., leukemia, non-small cell lung cancer, colon cancer, CNS cancer, melanoma, ovarian cancer, breast cancer, renal cancer, and prostate cancer) and disorders associated with angiogenesis function (e.g., age-related macular degeneration, macular dystrophy, and diabetes), a method of monitoring the treatment, a method of profiling gene expression, and a method of modulating cell growth, cell cycle, apoptosis, or gene expression.\n",
      "Patent #2 \n",
      "Substituted 2-thio-3,5-dicyano-4-aryl-6-aminopyridines and their use\n",
      "The invention relates to compounds of general formula (I), a method for the production thereof and the use thereof as pharmacologically effective substances for a broad medical indication spectrum. Furthermore, selective adenosine receptor ligands, preferably selective adenosine A1-, adenosine A2a- and/or adenosine A2b-receptor ligands are provided for the prophylaxis and/or the treatment of diseases, especially cardiovascular diseases, diseases of the urogenital region, diseases of the respiratory tract, inflammatory and neuroinflammatory diseases, diabetes, especially pancreatic diabetes, neurodegenerative diseases, pain states, cancer as well as liver fibrosis and liver cirrhosis.\n",
      "Patent #3 \n",
      "Anti-cd26 antibodies and methods of use thereof\n",
      "The present invention provides novel anti-CD26 antibodies and other, related polypeptides, as well as novel polynucleotides encoding the antibodies and polypeptides. The invention also provides methods of making the antibodies and polypeptides. Compositions and cells comprising the antibodies or polypeptides are further provided. Methods of using the antibodies and/or polypeptides, such as to inhibit cell proliferation and in the treatment of conditions associated with CD26, are also provided.\n",
      "Patent #4 \n",
      "Piperididinyl and N-amidinopiperidinyl derivatives\n",
      "This invention is directed to a compound of formula I which is useful for inhibiting the activity of Factor Xa, by combining said compound with a composition containing Factor Xa. The present invention is also directed to compositions containing compounds of the formula I, methods for their preparation, their use, such as in inhibiting the formation of thrombin or for treating a patient suffering from, or subject to, a disease state associated with a physiologically detrimental excess amount of thrombin.\n",
      "Patent #5 \n",
      "Modulators of ATP-binding cassette transporters\n",
      "The present invention relates to modulators of ATP-Binding Cassette (ÛÏABCÛ) transporters or fragments thereof, including Cystic Fibrosis Transmembrane Conductance Regulator, compositions thereof, and methods therewith. The present invention also relates to methods of treating ABC transporter mediated diseases using such modulators.\n"
     ]
    }
   ],
   "source": [
    "# Inspecting some of the reviews\n",
    "for i in range(5):\n",
    "    print(\"Patent #%d \"%(i+1))\n",
    "    print(reviews.title[i])\n",
    "    print(reviews.abstract[i])\n",
    "    print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A list of contractions from http://stackoverflow.com/questions/19790188/expanding-english-language-contractions-in-python\n",
    "contractions = { \n",
    "\"ain't\": \"am not\",\n",
    "\"aren't\": \"are not\",\n",
    "\"can't\": \"cannot\",\n",
    "\"can't've\": \"cannot have\",\n",
    "\"'cause\": \"because\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\n",
    "\"don't\": \"do not\",\n",
    "\"hadn't\": \"had not\",\n",
    "\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\n",
    "\"he'd\": \"he would\",\n",
    "\"he'd've\": \"he would have\",\n",
    "\"he'll\": \"he will\",\n",
    "\"he's\": \"he is\",\n",
    "\"how'd\": \"how did\",\n",
    "\"how'll\": \"how will\",\n",
    "\"how's\": \"how is\",\n",
    "\"i'd\": \"i would\",\n",
    "\"i'll\": \"i will\",\n",
    "\"i'm\": \"i am\",\n",
    "\"i've\": \"i have\",\n",
    "\"isn't\": \"is not\",\n",
    "\"it'd\": \"it would\",\n",
    "\"it'll\": \"it will\",\n",
    "\"it's\": \"it is\",\n",
    "\"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\",\n",
    "\"mayn't\": \"may not\",\n",
    "\"might've\": \"might have\",\n",
    "\"mightn't\": \"might not\",\n",
    "\"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\",\n",
    "\"needn't\": \"need not\",\n",
    "\"oughtn't\": \"ought not\",\n",
    "\"shan't\": \"shall not\",\n",
    "\"sha'n't\": \"shall not\",\n",
    "\"she'd\": \"she would\",\n",
    "\"she'll\": \"she will\",\n",
    "\"she's\": \"she is\",\n",
    "\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\n",
    "\"that'd\": \"that would\",\n",
    "\"that's\": \"that is\",\n",
    "\"there'd\": \"there had\",\n",
    "\"there's\": \"there is\",\n",
    "\"they'd\": \"they would\",\n",
    "\"they'll\": \"they will\",\n",
    "\"they're\": \"they are\",\n",
    "\"they've\": \"they have\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'd\": \"we would\",\n",
    "\"we'll\": \"we will\",\n",
    "\"we're\": \"we are\",\n",
    "\"we've\": \"we have\",\n",
    "\"weren't\": \"were not\",\n",
    "\"what'll\": \"what will\",\n",
    "\"what're\": \"what are\",\n",
    "\"what's\": \"what is\",\n",
    "\"what've\": \"what have\",\n",
    "\"where'd\": \"where did\",\n",
    "\"where's\": \"where is\",\n",
    "\"who'll\": \"who will\",\n",
    "\"who's\": \"who is\",\n",
    "\"won't\": \"will not\",\n",
    "\"wouldn't\": \"would not\",\n",
    "\"you'd\": \"you would\",\n",
    "\"you'll\": \"you will\",\n",
    "\"you're\": \"you are\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text, remove_stopwords = True):\n",
    "    '''Remove unwanted characters, stopwords, and format the text to create fewer nulls word embeddings'''\n",
    "    \n",
    "    # Convert words to lower case\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Replace contractions with their longer forms \n",
    "    if True:\n",
    "        text = text.split()\n",
    "        new_text = []\n",
    "        for word in text:\n",
    "            if word in contractions:\n",
    "                new_text.append(contractions[word])\n",
    "            else:\n",
    "                new_text.append(word)\n",
    "        text = \" \".join(new_text)\n",
    "    \n",
    "    # Format words and remove unwanted characters\n",
    "    text = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'\\<a href', ' ', text)\n",
    "    text = re.sub(r'&amp;', '', text) \n",
    "    text = re.sub(r'[_\"\\-;%()|+&=*%.,!?:#$@\\[\\]/]', ' ', text)\n",
    "    text = re.sub(r'<br />', ' ', text)\n",
    "    text = re.sub(r'\\'', ' ', text)\n",
    "    \n",
    "    # Optionally, remove stop words\n",
    "    if remove_stopwords:\n",
    "        text = text.split()\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        text = [w for w in text if not w in stops]\n",
    "        text = \" \".join(text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texts are complete.\n",
      "Summaries are complete.\n"
     ]
    }
   ],
   "source": [
    "# Clean the summaries and texts\n",
    "clean_summaries = []\n",
    "for title in reviews.title:\n",
    "    clean_summaries.append(clean_text(title, remove_stopwords=False))\n",
    "print(\"Texts are complete.\")\n",
    "\n",
    "\n",
    "clean_texts = []\n",
    "\n",
    "for abstract in reviews.abstract:\n",
    "    clean_texts.append(clean_text(abstract))\n",
    "print(\"Summaries are complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patent #1 \n",
      "novel compounds for treatment of cancer and disorders associated with angiogenesis function\n",
      "novel compounds treatment cancer disorders associated angiogenesis function also disclosed method preparing compounds pharmaceutical compositions packaged products containing compounds method using molecules treat cancer e g leukemia non small cell lung cancer colon cancer cns cancer melanoma ovarian cancer breast cancer renal cancer prostate cancer disorders associated angiogenesis function e g age related macular degeneration macular dystrophy diabetes method monitoring treatment method profiling gene expression method modulating cell growth cell cycle apoptosis gene expression\n",
      "Patent #2 \n",
      "substituted 2 thio 3 5 dicyano 4 aryl 6 aminopyridines and their use\n",
      "invention relates compounds general formula method production thereof use thereof pharmacologically effective substances broad medical indication spectrum furthermore selective adenosine receptor ligands preferably selective adenosine a1 adenosine a2a adenosine a2b receptor ligands provided prophylaxis treatment diseases especially cardiovascular diseases diseases urogenital region diseases respiratory tract inflammatory neuroinflammatory diseases diabetes especially pancreatic diabetes neurodegenerative diseases pain states cancer well liver fibrosis liver cirrhosis\n",
      "Patent #3 \n",
      "anti cd26 antibodies and methods of use thereof\n",
      "present invention provides novel anti cd26 antibodies related polypeptides well novel polynucleotides encoding antibodies polypeptides invention also provides methods making antibodies polypeptides compositions cells comprising antibodies polypeptides provided methods using antibodies polypeptides inhibit cell proliferation treatment conditions associated cd26 also provided\n",
      "Patent #4 \n",
      "piperididinyl and n amidinopiperidinyl derivatives\n",
      "invention directed compound formula useful inhibiting activity factor xa combining said compound composition containing factor xa present invention also directed compositions containing compounds formula methods preparation use inhibiting formation thrombin treating patient suffering subject disease state associated physiologically detrimental excess amount thrombin\n",
      "Patent #5 \n",
      "modulators of atp binding cassette transporters\n",
      "present invention relates modulators atp binding cassette ûïabcû transporters fragments thereof including cystic fibrosis transmembrane conductance regulator compositions thereof methods therewith present invention also relates methods treating abc transporter mediated diseases using modulators\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(\"Patent #%d \"%(i+1))\n",
    "    print(clean_summaries[i])\n",
    "    print(clean_texts[i])\n",
    "    print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words(count_dict, text):\n",
    "    '''Count the number of occurrences of each word in a set of text'''\n",
    "    for sentence in text:\n",
    "        for word in sentence.split():\n",
    "            if word not in count_dict:\n",
    "                count_dict[word] = 1\n",
    "            else:\n",
    "                count_dict[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of Vocabulary: 23187\n"
     ]
    }
   ],
   "source": [
    "# Find the number of times each word was used and the size of the vocabulary\n",
    "word_counts = {}\n",
    "\n",
    "count_words(word_counts, clean_summaries)\n",
    "count_words(word_counts, clean_texts)\n",
    "            \n",
    "print(\"Size of Vocabulary:\", len(word_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word embeddings: 2196016\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "with open('/home/abhishekjindal/nlp/data/glove.840B.300d.txt') as f:\n",
    "    for line in f:\n",
    "        values = line.split(' ')\n",
    "        word = values[0]\n",
    "        embedding = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = embedding\n",
    "\n",
    "print('Word embeddings:', len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words missing from CN: 2419\n",
      "Percent of words that are missing from vocabulary: 10.4326%\n"
     ]
    }
   ],
   "source": [
    "# Find the number of words that are missing from CN, and are used more than our threshold.\n",
    "missing_words = 0\n",
    "threshold = 0\n",
    "\n",
    "for word, count in word_counts.items():\n",
    "    if count > threshold:\n",
    "        if word not in embeddings_index:\n",
    "            missing_words += 1\n",
    "            \n",
    "missing_ratio = round((1.0*missing_words/len(word_counts))*100,4)\n",
    "            \n",
    "print(\"Number of words missing from CN:\", missing_words)\n",
    "print(\"Percent of words that are missing from vocabulary: {}%\".format(missing_ratio))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of unique words: 23187\n",
      "Number of words we will use: 23191\n",
      "Percent of words we will use: 100.02%\n"
     ]
    }
   ],
   "source": [
    "# Limit the vocab that we will use to words that appear ≥ threshold or are in GloVe\n",
    "\n",
    "#dictionary to convert words to integers\n",
    "vocab_to_int = {} \n",
    "\n",
    "value = 0\n",
    "for word, count in word_counts.items():\n",
    "    if count >= threshold or word in embeddings_index:\n",
    "        vocab_to_int[word] = value\n",
    "        value += 1\n",
    "\n",
    "# Special tokens that will be added to our vocab\n",
    "codes = [\"<UNK>\",\"<PAD>\",\"<EOS>\",\"<GO>\"]   \n",
    "\n",
    "# Add codes to vocab\n",
    "for code in codes:\n",
    "    vocab_to_int[code] = len(vocab_to_int)\n",
    "\n",
    "# Dictionary to convert integers to words\n",
    "int_to_vocab = {}\n",
    "for word, value in vocab_to_int.items():\n",
    "    int_to_vocab[value] = word\n",
    "\n",
    "usage_ratio = round(1.0*len(vocab_to_int) / len(word_counts),4)*100\n",
    "\n",
    "print(\"Total number of unique words:\", len(word_counts))\n",
    "print(\"Number of words we will use:\", len(vocab_to_int))\n",
    "print(\"Percent of words we will use: {}%\".format(usage_ratio))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23191\n"
     ]
    }
   ],
   "source": [
    "# Need to use 300 for embedding dimensions to match CN's vectors.\n",
    "embedding_dim = 300\n",
    "nb_words = len(vocab_to_int)\n",
    "\n",
    "# Create matrix with default values of zero\n",
    "word_embedding_matrix = np.zeros((nb_words, embedding_dim), dtype=np.float32)\n",
    "for word, i in vocab_to_int.items():\n",
    "    if word in embeddings_index:\n",
    "        word_embedding_matrix[i] = embeddings_index[word]\n",
    "    else:\n",
    "        # If word not in CN, create a random embedding for it\n",
    "        new_embedding = np.array(np.random.uniform(-1.0, 1.0, embedding_dim))\n",
    "        embeddings_index[word] = new_embedding\n",
    "        word_embedding_matrix[i] = new_embedding\n",
    "\n",
    "# Check if value matches len(vocab_to_int)\n",
    "print(len(word_embedding_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_ints(text, word_count, unk_count, eos=False):\n",
    "    '''Convert words in text to an integer.\n",
    "       If word is not in vocab_to_int, use UNK's integer.\n",
    "       Total the number of words and UNKs.\n",
    "       Add EOS token to the end of texts'''\n",
    "    ints = []\n",
    "    for sentence in text:\n",
    "        sentence_ints = []\n",
    "        for word in sentence.split():\n",
    "            word_count += 1\n",
    "            if word in vocab_to_int:\n",
    "                sentence_ints.append(vocab_to_int[word])\n",
    "            else:\n",
    "                sentence_ints.append(vocab_to_int[\"<UNK>\"])\n",
    "                unk_count += 1\n",
    "        if eos:\n",
    "            sentence_ints.append(vocab_to_int[\"<EOS>\"])\n",
    "        ints.append(sentence_ints)\n",
    "    return ints, word_count, unk_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of words in headlines: 610988\n",
      "Total number of UNKs in headlines: 0\n",
      "Percent of words that are UNK: 0.0%\n"
     ]
    }
   ],
   "source": [
    "# Apply convert_to_ints to clean_summaries and clean_texts\n",
    "word_count = 0\n",
    "unk_count = 0\n",
    "\n",
    "int_summaries, word_count, unk_count = convert_to_ints(clean_summaries, word_count, unk_count)\n",
    "int_texts, word_count, unk_count = convert_to_ints(clean_texts, word_count, unk_count, eos=True)\n",
    "\n",
    "unk_percent = round(1.0*unk_count/word_count,4)*100\n",
    "\n",
    "print(\"Total number of words in headlines:\", word_count)\n",
    "print(\"Total number of UNKs in headlines:\", unk_count)\n",
    "print(\"Percent of words that are UNK: {}%\".format(unk_percent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lengths(text):\n",
    "    '''Create a data frame of the sentence lengths from a text'''\n",
    "    lengths = []\n",
    "    for sentence in text:\n",
    "        lengths.append(len(sentence))\n",
    "    return pd.DataFrame(lengths, columns=['counts'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summaries:\n",
      "            counts\n",
      "count  8000.000000\n",
      "mean      8.096875\n",
      "std       4.597949\n",
      "min       1.000000\n",
      "25%       5.000000\n",
      "50%       7.000000\n",
      "75%      11.000000\n",
      "max      48.000000\n",
      "\n",
      "Texts:\n",
      "            counts\n",
      "count  8000.000000\n",
      "mean     69.276625\n",
      "std      29.684301\n",
      "min       5.000000\n",
      "25%      48.000000\n",
      "50%      67.000000\n",
      "75%      86.000000\n",
      "max     389.000000\n"
     ]
    }
   ],
   "source": [
    "lengths_summaries = create_lengths(int_summaries)\n",
    "lengths_texts = create_lengths(int_texts)\n",
    "\n",
    "print(\"Summaries:\")\n",
    "print(lengths_summaries.describe())\n",
    "print()\n",
    "print(\"Texts:\")\n",
    "print(lengths_texts.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105.0\n",
      "122.0\n",
      "154.0\n"
     ]
    }
   ],
   "source": [
    "# Inspect the length of texts\n",
    "print(np.percentile(lengths_texts.counts, 90))\n",
    "print(np.percentile(lengths_texts.counts, 95))\n",
    "print(np.percentile(lengths_texts.counts, 99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.0\n",
      "17.0\n",
      "23.0\n"
     ]
    }
   ],
   "source": [
    "# Inspect the length of summaries\n",
    "print(np.percentile(lengths_summaries.counts, 90))\n",
    "print(np.percentile(lengths_summaries.counts, 95))\n",
    "print(np.percentile(lengths_summaries.counts, 99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unk_counter(sentence):\n",
    "    '''Counts the number of time UNK appears in a sentence.'''\n",
    "    unk_count = 0\n",
    "    for word in sentence:\n",
    "        if word == vocab_to_int[\"<UNK>\"]:\n",
    "            unk_count += 1\n",
    "    return unk_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7945\n",
      "7945\n"
     ]
    }
   ],
   "source": [
    "# Sort the summaries and texts by the length of the texts, shortest to longest\n",
    "# Limit the length of summaries and texts based on the min and max ranges.\n",
    "# Remove reviews that include too many UNKs\n",
    "\n",
    "sorted_summaries = []\n",
    "sorted_texts = []\n",
    "max_text_length = 1000\n",
    "max_summary_length = 100\n",
    "min_length = 2\n",
    "unk_text_limit = 7\n",
    "unk_summary_limit = 3\n",
    "\n",
    "for length in range(min(lengths_texts.counts), max_text_length): \n",
    "    for count, words in enumerate(int_summaries):\n",
    "        if (len(int_summaries[count]) >= min_length and\n",
    "            len(int_summaries[count]) <= max_summary_length and\n",
    "            len(int_texts[count]) >= min_length and\n",
    "            unk_counter(int_summaries[count]) <= unk_summary_limit and\n",
    "            unk_counter(int_texts[count]) <= unk_text_limit and\n",
    "            length == len(int_texts[count])\n",
    "           ):\n",
    "            sorted_summaries.append(int_summaries[count])\n",
    "            sorted_texts.append(int_texts[count])\n",
    "        \n",
    "# Compare lengths to ensure they match\n",
    "print(len(sorted_summaries))\n",
    "print(len(sorted_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_inputs():\n",
    "    '''Create palceholders for inputs to the model'''\n",
    "    \n",
    "    input_data = tf.placeholder(tf.int32, [None, None], name='input')\n",
    "    targets = tf.placeholder(tf.int32, [None, None], name='targets')\n",
    "    lr = tf.placeholder(tf.float32, name='learning_rate')\n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "    summary_length = tf.placeholder(tf.int32, (None,), name='summary_length')\n",
    "    max_summary_length = tf.reduce_max(summary_length, name='max_dec_len')\n",
    "    text_length = tf.placeholder(tf.int32, (None,), name='text_length')\n",
    "\n",
    "    return input_data, targets, lr, keep_prob, summary_length, max_summary_length, text_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_encoding_input(target_data, vocab_to_int, batch_size):\n",
    "    '''Remove the last word id from each batch and concat the <GO> to the begining of each batch'''\n",
    "    \n",
    "    ending = tf.strided_slice(target_data, [0, 0], [batch_size, -1], [1, 1])\n",
    "    dec_input = tf.concat([tf.fill([batch_size, 1], vocab_to_int['<GO>']), ending], 1)\n",
    "\n",
    "    return dec_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoding_layer(rnn_size, sequence_length, num_layers, rnn_inputs, keep_prob):\n",
    "    '''Create the encoding layer'''\n",
    "    \n",
    "    for layer in range(num_layers):\n",
    "        with tf.variable_scope('encoder_{}'.format(layer)):\n",
    "            cell_fw = tf.contrib.rnn.GRUCell(rnn_size)\n",
    "            cell_fw = tf.contrib.rnn.DropoutWrapper(cell_fw, \n",
    "                                                    input_keep_prob = keep_prob)\n",
    "\n",
    "            cell_bw = tf.contrib.rnn.GRUCell(rnn_size)\n",
    "            cell_bw = tf.contrib.rnn.DropoutWrapper(cell_bw, \n",
    "                                                    input_keep_prob = keep_prob)\n",
    "\n",
    "            enc_output, enc_state = tf.nn.bidirectional_dynamic_rnn(cell_fw, \n",
    "                                                                    cell_bw, \n",
    "                                                                    rnn_inputs,\n",
    "                                                                    sequence_length,\n",
    "                                                                    dtype=tf.float32)\n",
    "    # Join outputs since we are using a bidirectional RNN\n",
    "    enc_output = tf.concat(enc_output,2)\n",
    "    \n",
    "    return enc_output, enc_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_decoding_layer(dec_embed_input, summary_length, dec_cell, initial_state, output_layer, \n",
    "                            vocab_size, max_summary_length):\n",
    "    '''Create the training logits'''\n",
    "    \n",
    "    training_helper = tf.contrib.seq2seq.TrainingHelper(inputs=dec_embed_input,\n",
    "                                                        sequence_length=summary_length,\n",
    "                                                        time_major=False)\n",
    "\n",
    "    training_decoder = tf.contrib.seq2seq.BasicDecoder(dec_cell,\n",
    "                                                       training_helper,\n",
    "                                                       initial_state,\n",
    "                                                       output_layer) \n",
    "\n",
    "    training_logits, _ = tf.contrib.seq2seq.dynamic_decode(training_decoder,\n",
    "                                                           output_time_major=False,\n",
    "                                                           impute_finished=True,\n",
    "                                                           maximum_iterations=max_summary_length)\n",
    "    return training_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_decoding_layer(embeddings, start_token, end_token, dec_cell, initial_state, output_layer,\n",
    "                             max_summary_length, batch_size):\n",
    "    '''Create the inference logits'''\n",
    "    \n",
    "    start_tokens = tf.tile(tf.constant([start_token], dtype=tf.int32), [batch_size], name='start_tokens')\n",
    "    \n",
    "    inference_helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(embeddings,\n",
    "                                                                start_tokens,\n",
    "                                                                end_token)\n",
    "                \n",
    "    inference_decoder = tf.contrib.seq2seq.BasicDecoder(dec_cell,\n",
    "                                                        inference_helper,\n",
    "                                                        initial_state,\n",
    "                                                        output_layer)\n",
    "                \n",
    "    inference_logits, _ = tf.contrib.seq2seq.dynamic_decode(inference_decoder,\n",
    "                                                            output_time_major=False,\n",
    "                                                            impute_finished=True,\n",
    "                                                            maximum_iterations=max_summary_length)\n",
    "    \n",
    "    return inference_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoding_layer(dec_embed_input, embeddings, enc_output, enc_state, vocab_size, text_length, summary_length, \n",
    "                   max_summary_length, rnn_size, vocab_to_int, keep_prob, batch_size, num_layers):\n",
    "    '''Create the decoding cell and attention for the training and inference decoding layers'''\n",
    "    \n",
    "    for layer in range(num_layers):\n",
    "        with tf.variable_scope('decoder_{}'.format(layer)):\n",
    "            gru = tf.contrib.rnn.GRUCell(rnn_size)\n",
    "            dec_cell = tf.contrib.rnn.DropoutWrapper(gru, \n",
    "                                                     input_keep_prob = keep_prob)\n",
    "    \n",
    "    output_layer = Dense(vocab_size,\n",
    "                         kernel_initializer = tf.truncated_normal_initializer(mean = 0.0, stddev=0.1))\n",
    "    \n",
    "    attn_mech = tf.contrib.seq2seq.BahdanauAttention(rnn_size,\n",
    "                                                  enc_output,\n",
    "                                                  text_length,\n",
    "                                                  normalize=False,\n",
    "                                                  name='BahdanauAttention')\n",
    "\n",
    "    dec_cell = tf.contrib.seq2seq.DynamicAttentionWrapper(dec_cell,\n",
    "                                                          attn_mech,\n",
    "                                                          rnn_size)\n",
    "            \n",
    "    initial_state = tf.contrib.seq2seq.DynamicAttentionWrapperState(enc_state[0],\n",
    "                                                                    _zero_state_tensors(rnn_size, \n",
    "                                                                                        batch_size, \n",
    "                                                                                        tf.float32)) \n",
    "    with tf.variable_scope(\"decode\"):\n",
    "        training_logits = training_decoding_layer(dec_embed_input, \n",
    "                                                  summary_length, \n",
    "                                                  dec_cell, \n",
    "                                                  initial_state,\n",
    "                                                  output_layer,\n",
    "                                                  vocab_size, \n",
    "                                                  max_summary_length)\n",
    "    with tf.variable_scope(\"decode\", reuse=True):\n",
    "        inference_logits = inference_decoding_layer(embeddings,  \n",
    "                                                    vocab_to_int['<GO>'], \n",
    "                                                    vocab_to_int['<EOS>'],\n",
    "                                                    dec_cell, \n",
    "                                                    initial_state, \n",
    "                                                    output_layer,\n",
    "                                                    max_summary_length,\n",
    "                                                    batch_size)\n",
    "\n",
    "    return training_logits, inference_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq2seq_model(input_data, target_data, keep_prob, text_length, summary_length, max_summary_length, \n",
    "                  vocab_size, rnn_size, num_layers, vocab_to_int, batch_size):\n",
    "    '''Use the previous functions to create the training and inference logits'''\n",
    "    \n",
    "    # Use Numberbatch's embeddings and the newly created ones as our embeddings\n",
    "    embeddings = word_embedding_matrix\n",
    "    \n",
    "    enc_embed_input = tf.nn.embedding_lookup(embeddings, input_data)\n",
    "    enc_output, enc_state = encoding_layer(rnn_size, text_length, num_layers, enc_embed_input, keep_prob)\n",
    "    \n",
    "    dec_input = process_encoding_input(target_data, vocab_to_int, batch_size)\n",
    "    dec_embed_input = tf.nn.embedding_lookup(embeddings, dec_input)\n",
    "    \n",
    "    training_logits, inference_logits  = decoding_layer(dec_embed_input, \n",
    "                                                        embeddings,\n",
    "                                                        enc_output,\n",
    "                                                        enc_state, \n",
    "                                                        vocab_size, \n",
    "                                                        text_length, \n",
    "                                                        summary_length, \n",
    "                                                        max_summary_length,\n",
    "                                                        rnn_size, \n",
    "                                                        vocab_to_int, \n",
    "                                                        keep_prob, \n",
    "                                                        batch_size,\n",
    "                                                        num_layers)\n",
    "    \n",
    "    return training_logits, inference_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sentence_batch(sentence_batch):\n",
    "    \"\"\"Pad sentences with <PAD> so that each sentence of a batch has the same length\"\"\"\n",
    "    max_sentence = max([len(sentence) for sentence in sentence_batch])\n",
    "    return [sentence + [vocab_to_int['<PAD>']] * (max_sentence - len(sentence)) for sentence in sentence_batch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(summaries, texts, batch_size):\n",
    "    \"\"\"Batch summaries, texts, and the lengths of their sentences together\"\"\"\n",
    "    for batch_i in range(0, len(texts)//batch_size):\n",
    "        start_i = batch_i * batch_size\n",
    "        summaries_batch = summaries[start_i:start_i + batch_size]\n",
    "        texts_batch = texts[start_i:start_i + batch_size]\n",
    "        pad_summaries_batch = np.array(pad_sentence_batch(summaries_batch))\n",
    "        pad_texts_batch = np.array(pad_sentence_batch(texts_batch))\n",
    "        \n",
    "        # Need the lengths for the _lengths parameters\n",
    "        pad_summaries_lengths = []\n",
    "        for summary in pad_summaries_batch:\n",
    "            pad_summaries_lengths.append(len(summary))\n",
    "        \n",
    "        pad_texts_lengths = []\n",
    "        for text in pad_texts_batch:\n",
    "            pad_texts_lengths.append(len(text))\n",
    "        \n",
    "        yield pad_summaries_batch, pad_texts_batch, pad_summaries_lengths, pad_texts_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the Hyperparameters\n",
    "epochs = 15\n",
    "batch_size = 64\n",
    "rnn_size = 256\n",
    "num_layers = 2\n",
    "learning_rate = 0.005\n",
    "keep_probability = 0.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph is built.\n"
     ]
    }
   ],
   "source": [
    "# Build the graph\n",
    "train_graph = tf.Graph()\n",
    "# Set the graph to default to ensure that it is ready for training\n",
    "with train_graph.as_default():\n",
    "    \n",
    "    # Load the model inputs    \n",
    "    input_data, targets, lr, keep_prob, summary_length, max_summary_length, text_length = model_inputs()\n",
    "\n",
    "    # Create the training and inference logits\n",
    "    training_logits, inference_logits = seq2seq_model(tf.reverse(input_data, [-1]),\n",
    "                                                      targets, \n",
    "                                                      keep_prob,   \n",
    "                                                      text_length,\n",
    "                                                      summary_length,\n",
    "                                                      max_summary_length,\n",
    "                                                      len(vocab_to_int)+1,\n",
    "                                                      rnn_size, \n",
    "                                                      num_layers, \n",
    "                                                      vocab_to_int,\n",
    "                                                      batch_size)\n",
    "    \n",
    "    # Create tensors for the training logits and inference logits\n",
    "    training_logits = tf.identity(training_logits.rnn_output, 'logits')\n",
    "    inference_logits = tf.identity(inference_logits.sample_id, name='predictions')\n",
    "    \n",
    "    # Create the weights for sequence_loss\n",
    "    masks = tf.sequence_mask(summary_length, max_summary_length, dtype=tf.float32, name='masks')\n",
    "\n",
    "    with tf.name_scope(\"optimization\"):\n",
    "        # Loss function\n",
    "        cost = tf.contrib.seq2seq.sequence_loss(\n",
    "            training_logits,\n",
    "            targets,\n",
    "            masks)\n",
    "\n",
    "        # Optimizer\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "\n",
    "        # Gradient Clipping\n",
    "        gradients = optimizer.compute_gradients(cost)\n",
    "        capped_gradients = [(tf.clip_by_value(grad, -5., 5.), var) for grad, var in gradients if grad is not None]\n",
    "        train_op = optimizer.apply_gradients(capped_gradients)\n",
    "print(\"Graph is built.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7945\n",
      "The shortest text length: 5\n",
      "The longest text length: 77\n"
     ]
    }
   ],
   "source": [
    "# Subset the data for training\n",
    "start = 0#200000\n",
    "end = start + 5000\n",
    "sorted_summaries_short = sorted_summaries[start:end]\n",
    "sorted_texts_short = sorted_texts[start:end]\n",
    "print(len(sorted_summaries))\n",
    "print(\"The shortest text length:\", len(sorted_texts_short[0]))\n",
    "print(\"The longest text length:\",len(sorted_texts_short[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n",
      "Epoch   1/15 Batch   20/78 - Loss:  4.019, Seconds: 56.72\n",
      "Average loss for this update: 3.842\n",
      "New Record!\n",
      "Epoch   1/15 Batch   40/78 - Loss:  2.904, Seconds: 47.58\n",
      "Average loss for this update: 2.704\n",
      "New Record!\n",
      "Epoch   1/15 Batch   60/78 - Loss:  2.551, Seconds: 69.25\n",
      "Average loss for this update: 2.54\n",
      "New Record!\n",
      "Epoch   2/15 Batch   20/78 - Loss:  2.423, Seconds: 56.37\n",
      "Average loss for this update: 2.415\n",
      "New Record!\n",
      "Epoch   2/15 Batch   40/78 - Loss:  2.240, Seconds: 48.06\n",
      "Average loss for this update: 2.119\n",
      "New Record!\n",
      "Epoch   2/15 Batch   60/78 - Loss:  2.035, Seconds: 69.34\n",
      "Average loss for this update: 2.021\n",
      "New Record!\n",
      "Epoch   3/15 Batch   20/78 - Loss:  2.002, Seconds: 55.94\n",
      "Average loss for this update: 1.995\n",
      "New Record!\n",
      "Epoch   3/15 Batch   40/78 - Loss:  1.860, Seconds: 47.40\n",
      "Average loss for this update: 1.768\n",
      "New Record!\n",
      "Epoch   3/15 Batch   60/78 - Loss:  1.701, Seconds: 69.21\n",
      "Average loss for this update: 1.68\n",
      "New Record!\n",
      "Epoch   4/15 Batch   20/78 - Loss:  1.658, Seconds: 56.24\n",
      "Average loss for this update: 1.649\n",
      "New Record!\n",
      "Epoch   4/15 Batch   40/78 - Loss:  1.558, Seconds: 47.98\n",
      "Average loss for this update: 1.505\n",
      "New Record!\n",
      "Epoch   4/15 Batch   60/78 - Loss:  1.459, Seconds: 68.57\n",
      "Average loss for this update: 1.451\n",
      "New Record!\n",
      "Epoch   5/15 Batch   20/78 - Loss:  1.426, Seconds: 56.18\n",
      "Average loss for this update: 1.427\n",
      "New Record!\n",
      "Epoch   5/15 Batch   40/78 - Loss:  1.380, Seconds: 47.64\n",
      "Average loss for this update: 1.332\n",
      "New Record!\n",
      "Epoch   5/15 Batch   60/78 - Loss:  1.313, Seconds: 69.66\n",
      "Average loss for this update: 1.308\n",
      "New Record!\n",
      "Epoch   6/15 Batch   20/78 - Loss:  1.288, Seconds: 56.68\n",
      "Average loss for this update: 1.291\n",
      "New Record!\n",
      "Epoch   6/15 Batch   40/78 - Loss:  1.267, Seconds: 47.41\n",
      "Average loss for this update: 1.226\n",
      "New Record!\n",
      "Epoch   6/15 Batch   60/78 - Loss:  1.199, Seconds: 69.30\n",
      "Average loss for this update: 1.201\n",
      "New Record!\n",
      "Epoch   7/15 Batch   20/78 - Loss:  1.181, Seconds: 56.53\n",
      "Average loss for this update: 1.176\n",
      "New Record!\n",
      "Epoch   7/15 Batch   40/78 - Loss:  1.146, Seconds: 47.54\n",
      "Average loss for this update: 1.126\n",
      "New Record!\n",
      "Epoch   7/15 Batch   60/78 - Loss:  1.118, Seconds: 69.45\n",
      "Average loss for this update: 1.126\n",
      "No Improvement.\n",
      "Epoch   8/15 Batch   20/78 - Loss:  1.086, Seconds: 56.09\n",
      "Average loss for this update: 1.078\n",
      "New Record!\n",
      "Epoch   8/15 Batch   40/78 - Loss:  1.069, Seconds: 47.67\n",
      "Average loss for this update: 1.059\n",
      "New Record!\n",
      "Epoch   8/15 Batch   60/78 - Loss:  1.049, Seconds: 69.58\n",
      "Average loss for this update: 1.059\n",
      "New Record!\n",
      "Epoch   9/15 Batch   20/78 - Loss:  1.037, Seconds: 56.08\n",
      "Average loss for this update: 1.035\n",
      "New Record!\n",
      "Epoch   9/15 Batch   40/78 - Loss:  1.015, Seconds: 47.39\n",
      "Average loss for this update: 1.001\n",
      "New Record!\n",
      "Epoch   9/15 Batch   60/78 - Loss:  1.003, Seconds: 69.26\n",
      "Average loss for this update: 1.019\n",
      "No Improvement.\n",
      "Epoch  10/15 Batch   20/78 - Loss:  0.992, Seconds: 55.12\n",
      "Average loss for this update: 0.989\n",
      "New Record!\n",
      "Epoch  10/15 Batch   40/78 - Loss:  0.973, Seconds: 46.97\n",
      "Average loss for this update: 0.972\n",
      "New Record!\n",
      "Epoch  10/15 Batch   60/78 - Loss:  0.981, Seconds: 67.72\n",
      "Average loss for this update: 1.0\n",
      "No Improvement.\n",
      "Epoch  11/15 Batch   20/78 - Loss:  0.977, Seconds: 55.39\n",
      "Average loss for this update: 0.98\n",
      "No Improvement.\n",
      "Epoch  11/15 Batch   40/78 - Loss:  0.974, Seconds: 46.29\n",
      "Average loss for this update: 0.95\n",
      "New Record!\n",
      "Epoch  11/15 Batch   60/78 - Loss:  0.947, Seconds: 67.83\n",
      "Average loss for this update: 0.968\n",
      "No Improvement.\n",
      "Epoch  12/15 Batch   20/78 - Loss:  0.959, Seconds: 54.69\n",
      "Average loss for this update: 0.956\n",
      "No Improvement.\n",
      "Epoch  12/15 Batch   40/78 - Loss:  0.938, Seconds: 46.79\n",
      "Average loss for this update: 0.928\n",
      "New Record!\n",
      "Epoch  12/15 Batch   60/78 - Loss:  0.928, Seconds: 66.77\n",
      "Average loss for this update: 0.942\n",
      "No Improvement.\n",
      "Epoch  13/15 Batch   20/78 - Loss:  0.958, Seconds: 54.29\n",
      "Average loss for this update: 0.959\n",
      "No Improvement.\n",
      "Epoch  13/15 Batch   40/78 - Loss:  0.962, Seconds: 46.31\n",
      "Average loss for this update: 0.949\n",
      "No Improvement.\n",
      "Stopping Training.\n"
     ]
    }
   ],
   "source": [
    "# Train the Model\n",
    "learning_rate_decay = 0.95\n",
    "min_learning_rate = 0.0005\n",
    "display_step = 20 # Check training loss after every 20 batches\n",
    "stop_early = 0 \n",
    "stop = 3 # If the update loss does not decrease in 3 consecutive update checks, stop training\n",
    "per_epoch = 3 # Make 3 update checks per epoch\n",
    "update_check = (len(sorted_texts_short)//batch_size//per_epoch)-1\n",
    "print(update_check)\n",
    "update_loss = 0 \n",
    "batch_loss = 0\n",
    "summary_update_loss = [] # Record the update losses for saving improvements in the model\n",
    "\n",
    "checkpoint = \"best_model.ckpt\" \n",
    "with tf.Session(graph=train_graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # If we want to continue training a previous session\n",
    "    #loader = tf.train.import_meta_graph(\"./\" + checkpoint + '.meta')\n",
    "    #loader.restore(sess, checkpoint)\n",
    "    \n",
    "    for epoch_i in range(1, epochs+1):\n",
    "        update_loss = 0\n",
    "        batch_loss = 0\n",
    "        for batch_i, (summaries_batch, texts_batch, summaries_lengths, texts_lengths) in enumerate(\n",
    "                get_batches(sorted_summaries_short, sorted_texts_short, batch_size)):\n",
    "            start_time = time.time()\n",
    "            _, loss = sess.run(\n",
    "                [train_op, cost],\n",
    "                {input_data: texts_batch,\n",
    "                 targets: summaries_batch,\n",
    "                 lr: learning_rate,\n",
    "                 summary_length: summaries_lengths,\n",
    "                 text_length: texts_lengths,\n",
    "                 keep_prob: keep_probability})\n",
    "\n",
    "            batch_loss += loss\n",
    "            update_loss += loss\n",
    "            end_time = time.time()\n",
    "            batch_time = end_time - start_time\n",
    "\n",
    "            if batch_i % display_step == 0 and batch_i > 0:\n",
    "                print('Epoch {:>3}/{} Batch {:>4}/{} - Loss: {:>6.3f}, Seconds: {:>4.2f}'\n",
    "                      .format(epoch_i,\n",
    "                              epochs, \n",
    "                              batch_i, \n",
    "                              len(sorted_texts_short) // batch_size, \n",
    "                              batch_loss / display_step, \n",
    "                              batch_time*display_step))\n",
    "                batch_loss = 0\n",
    "\n",
    "            if batch_i % update_check == 0 and batch_i > 0:\n",
    "                print(\"Average loss for this update:\", round(update_loss/update_check,3))\n",
    "                summary_update_loss.append(update_loss)\n",
    "                \n",
    "                # If the update loss is at a new minimum, save the model\n",
    "                if update_loss <= min(summary_update_loss):\n",
    "                    print('New Record!') \n",
    "                    stop_early = 0\n",
    "                    saver = tf.train.Saver() \n",
    "                    saver.save(sess, checkpoint)\n",
    "\n",
    "                else:\n",
    "                    print(\"No Improvement.\")\n",
    "                    stop_early += 1\n",
    "                    if stop_early == stop:\n",
    "                        break\n",
    "                update_loss = 0\n",
    "            \n",
    "                    \n",
    "        # Reduce learning rate, but not below its minimum value\n",
    "        learning_rate *= learning_rate_decay\n",
    "        if learning_rate < min_learning_rate:\n",
    "            learning_rate = min_learning_rate\n",
    "        \n",
    "        if stop_early == stop:\n",
    "            print(\"Stopping Training.\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_seq(text):\n",
    "    '''Prepare the text for the model'''\n",
    "    \n",
    "    text = clean_text(text)\n",
    "    return [vocab_to_int.get(word, vocab_to_int['<UNK>']) for word in text.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./best_model.ckpt\n",
      "Original Title: novel compounds for treatment of cancer and disorders associated with angiogenesis function\n",
      "\n",
      "Text\n",
      "  Word Ids:    [43, 21935, 6340, 4182, 10114, 10257, 1975, 623, 9448, 17979, 21594, 1921, 21935, 22123, 21690, 794, 17811, 220, 21935, 21594, 8214, 5831, 3724, 4182, 10765, 20356, 22719, 13319, 14200, 17518, 12874, 4182, 12125, 4182, 10974, 4182, 16551, 9139, 4182, 13248, 4182, 11975, 4182, 20808, 4182, 10114, 10257, 1975, 623, 10765, 20356, 5841, 62, 4827, 14803, 4827, 2894, 678, 21594, 548, 6340, 21594, 12075, 88, 18294, 21594, 1990, 17518, 7577, 17518, 13454, 12452, 88, 18294]\n",
      "  Input Words: novel compounds treatment cancer disorders associated angiogenesis function also disclosed method preparing compounds pharmaceutical compositions packaged products containing compounds method using molecules treat cancer e g leukemia non small cell lung cancer colon cancer cns cancer melanoma ovarian cancer breast cancer renal cancer prostate cancer disorders associated angiogenesis function e g age related macular degeneration macular dystrophy diabetes method monitoring treatment method profiling gene expression method modulating cell growth cell cycle apoptosis gene expression\n",
      "\n",
      "Summary\n",
      "  Word Ids:       [43, 8300, 15350, 4875, 10257, 1692, 8300, 10114]\n",
      "  Response Words: novel surgical remedies and associated of surgical disorders\n"
     ]
    }
   ],
   "source": [
    "# Create your own review or use one from the dataset\n",
    "#input_sentence = \"I have never eaten an apple before, but this red one was nice. \\\n",
    "                  #I think that I will try a green apple next time.\"\n",
    "#text = text_to_seq(input_sentence)\n",
    "title = \"novel compounds for treatment of cancer and disorders associated with angiogenesis function\"\n",
    " \n",
    "input_sentence= \"novel compounds treatment cancer disorders associated angiogenesis function also disclosed method preparing compounds pharmaceutical compositions packaged products containing compounds method using molecules treat cancer e g leukemia non small cell lung cancer colon cancer cns cancer melanoma ovarian cancer breast cancer renal cancer prostate cancer disorders associated angiogenesis function e g age related macular degeneration macular dystrophy diabetes method monitoring treatment method profiling gene expression method modulating cell growth cell cycle apoptosis gene expression\"\n",
    "text = text_to_seq(input_sentence)\n",
    "#random = np.random.randint(0,len(clean_texts))\n",
    "#input_sentence = clean_texts[random]\n",
    "#title = clean_summaries[random]\n",
    "#text = text_to_seq(clean_texts[random])\n",
    "\n",
    "checkpoint = \"./best_model.ckpt\"\n",
    "\n",
    "loaded_graph = tf.Graph()\n",
    "with tf.Session(graph=loaded_graph) as sess:\n",
    "    # Load saved model\n",
    "    loader = tf.train.import_meta_graph(checkpoint + '.meta')\n",
    "    loader.restore(sess, checkpoint)\n",
    "\n",
    "    input_data = loaded_graph.get_tensor_by_name('input:0')\n",
    "    logits = loaded_graph.get_tensor_by_name('predictions:0')\n",
    "    text_length = loaded_graph.get_tensor_by_name('text_length:0')\n",
    "    summary_length = loaded_graph.get_tensor_by_name('summary_length:0')\n",
    "    keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
    "    \n",
    "    #Multiply by batch_size to match the model's input parameters\n",
    "    answer_logits = sess.run(logits, {input_data: [text]*batch_size, \n",
    "                                      summary_length: [np.random.randint(10,20)], \n",
    "                                      text_length: [len(text)]*batch_size,\n",
    "                                      keep_prob: 1.0})[0] \n",
    "\n",
    "# Remove the padding from the tweet\n",
    "pad = vocab_to_int[\"<PAD>\"] \n",
    "\n",
    "print('Original Title:', title)\n",
    "\n",
    "print('\\nText')\n",
    "print('  Word Ids:    {}'.format([i for i in text]))\n",
    "print('  Input Words: {}'.format(\" \".join([int_to_vocab[i] for i in text])))\n",
    "\n",
    "print('\\nSummary')\n",
    "print('  Word Ids:       {}'.format([i for i in answer_logits if i != pad]))\n",
    "print('  Response Words: {}'.format(\" \".join([int_to_vocab[i] for i in answer_logits if i != pad])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'rouge-l': {'p': 0.5, 'f': 0.4999999999995, 'r': 0.5}, 'rouge-2': {'p': 0.3333333333333333, 'f': 0.3333333283333334, 'r': 0.3333333333333333}, 'rouge-1': {'p': 1.0, 'f': 0.999999995, 'r': 1.0}}]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from rouge import Rouge\n",
    "\n",
    "hyps = \"this is a test\"\n",
    "refs = \"a test is this\"\n",
    "rouge = Rouge()\n",
    "scores = rouge.get_scores(hyps, refs)\n",
    "\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_text_top10 = clean_texts[0:100]\n",
    "clean_summaries_top10 = clean_summaries[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_summaries = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./best_model.ckpt\n",
      "Current iter:  0\n",
      "Current iter:  50\n"
     ]
    }
   ],
   "source": [
    "output_summaries = []\n",
    "checkpoint = \"./best_model.ckpt\"\n",
    "pad = vocab_to_int[\"<PAD>\"] \n",
    "loaded_graph = tf.Graph()\n",
    "with tf.Session(graph=loaded_graph) as sess:\n",
    "    # Load saved model\n",
    "    loader = tf.train.import_meta_graph(checkpoint + '.meta')\n",
    "    loader.restore(sess, checkpoint)\n",
    "\n",
    "    input_data = loaded_graph.get_tensor_by_name('input:0')\n",
    "    logits = loaded_graph.get_tensor_by_name('predictions:0')\n",
    "    text_length = loaded_graph.get_tensor_by_name('text_length:0')\n",
    "    summary_length = loaded_graph.get_tensor_by_name('summary_length:0')\n",
    "    keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
    "    \n",
    "    #Multiply by batch_size to match the model's input parameters\n",
    "    for i in range(0, len(clean_summaries_top10)):\n",
    "        text = text_to_seq(clean_text_top10[i])\n",
    "        answer_logits = sess.run(logits, {input_data: [text]*batch_size, \n",
    "                                      summary_length: [np.random.randint(10,20)], \n",
    "                                      text_length: [len(text)]*batch_size,\n",
    "                                      keep_prob: 1.0})[0] \n",
    "        temp = \" \".join([int_to_vocab[i] for i in answer_logits if i != pad])\n",
    "        output_summaries.append(temp)\n",
    "        if(i % 50 == 0):\n",
    "            print(\"Current iter: \", i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge-l': {'p': 0.3879940377819635, 'f': 0.34233405843126063, 'r': 0.4331827797761234}, 'rouge-2': {'p': 0.24807278832278828, 'f': 0.23464415085931237, 'r': 0.2820414805782453}, 'rouge-1': {'p': 0.5518480963480963, 'f': 0.4673171635400374, 'r': 0.475996474715515}}\n"
     ]
    }
   ],
   "source": [
    "scores = rouge.get_scores(output_summaries, clean_summaries_top10, avg = True)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adjustable panel with a selectively of selectively and display device\n",
      "slat assembly for blind\n"
     ]
    }
   ],
   "source": [
    "index = 59\n",
    "print(output_summaries[index])\n",
    "print(clean_summaries_top10[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "slat assembly blind includes plurality column like support rods board like support blocks alternatively arranged juxtaposition others adhesive layer coated bottom surface thereon woven fabric various diagrams attached thereto via adhesive layer thereof weaving spots woven fabric adjacent joints juxtaposed support rods blocks thereof securely bound together form rigid straight piece decoration article cut equal space plurality slat pieces cord passage hole disposed lateral sides thereon respectively form horizontal type slat piece hook hole disposed one lateral side thereon provide vertical type slat piece thereof thus slat pieces turned rolled alternatively arranged support rods blocks woven fabric thereof verify diagrams displayed thereon achieve special visual effects blind assembly thereof besides via weaving texture woven fabric slat pieces thereof equipped rich colors three dimensional diagrams well via adhesive layer woven fabric securely bound support rods blocks efficiently avoiding loose yarns cutting operation thereof\n"
     ]
    }
   ],
   "source": [
    "print(clean_text_top10[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir_test = \"/home/abhishekjindal/nlp/data/\"\n",
    "reviews_test = pd.read_csv(os.path.join(data_dir,\"results-20180309-000423_8000_after_2010.csv\"), encoding = \"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texts are complete.\n",
      "Summaries are complete.\n"
     ]
    }
   ],
   "source": [
    "# Clean the Test summaries and texts\n",
    "clean_summaries_test = []\n",
    "for title in reviews_test.title:\n",
    "    clean_summaries_test.append(clean_text(title, remove_stopwords=False))\n",
    "print(\"Texts are complete.\")\n",
    "\n",
    "\n",
    "clean_texts_test = []\n",
    "\n",
    "for abstract in reviews_test.abstract:\n",
    "    clean_texts_test.append(clean_text(abstract))\n",
    "print(\"Summaries are complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_text_test_sample = clean_texts_test[0:500]\n",
    "clean_summaries_test_sample = clean_summaries_test[0:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./best_model.ckpt\n",
      "Current iter:  0\n",
      "Current iter:  50\n",
      "Current iter:  100\n",
      "Current iter:  150\n",
      "Current iter:  200\n",
      "Current iter:  250\n",
      "Current iter:  300\n",
      "Current iter:  350\n",
      "Current iter:  400\n",
      "Current iter:  450\n"
     ]
    }
   ],
   "source": [
    "output_summaries_test = []\n",
    "checkpoint = \"./best_model.ckpt\"\n",
    "pad = vocab_to_int[\"<PAD>\"] \n",
    "loaded_graph = tf.Graph()\n",
    "with tf.Session(graph=loaded_graph) as sess:\n",
    "    # Load saved model\n",
    "    loader = tf.train.import_meta_graph(checkpoint + '.meta')\n",
    "    loader.restore(sess, checkpoint)\n",
    "\n",
    "    input_data = loaded_graph.get_tensor_by_name('input:0')\n",
    "    logits = loaded_graph.get_tensor_by_name('predictions:0')\n",
    "    text_length = loaded_graph.get_tensor_by_name('text_length:0')\n",
    "    summary_length = loaded_graph.get_tensor_by_name('summary_length:0')\n",
    "    keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
    "    \n",
    "    #Multiply by batch_size to match the model's input parameters\n",
    "    for i in range(0, len(clean_text_test_sample)):\n",
    "        text = text_to_seq(clean_text_test_sample[i])\n",
    "        answer_logits = sess.run(logits, {input_data: [text]*batch_size, \n",
    "                                      summary_length: [np.random.randint(10,20)], \n",
    "                                      text_length: [len(text)]*batch_size,\n",
    "                                      keep_prob: 1.0})[0] \n",
    "        temp = \" \".join([int_to_vocab[i] for i in answer_logits if i != pad])\n",
    "        output_summaries_test.append(temp)\n",
    "        if(i % 50 == 0):\n",
    "            print(\"Current iter: \", i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge-l': {'p': 0.2027214013798379, 'f': 0.1677374558365543, 'r': 0.20391192018648963}, 'rouge-2': {'p': 0.06628099678099679, 'f': 0.05846907016385713, 'r': 0.06109287592056354}, 'rouge-1': {'p': 0.267384854034854, 'f': 0.22847900400689353, 'r': 0.23093318275463787}}\n"
     ]
    }
   ],
   "source": [
    "scores = rouge.get_scores(output_summaries_test, clean_summaries_test_sample, avg = True)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4548019047027907\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "hypothesis = ['It', 'is', 'a', 'cat', 'at', 'room']\n",
    "reference = ['It', 'is', 'a', 'cat', 'inside', 'the', 'room']\n",
    "#there may be several references\n",
    "BLEUscore = nltk.translate.bleu_score.sentence_bleu([reference], hypothesis)\n",
    "print(BLEUscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_summaries_test_sample_v1 = []\n",
    "for i in range(0, len(clean_summaries_test_sample)):\n",
    "    clean_summaries_test_sample_v1.append([clean_summaries_test_sample[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.12005503739412351\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import SmoothingFunction, sentence_bleu\n",
    "cc = SmoothingFunction()\n",
    "BLEUscore = nltk.translate.bleu_score.sentence_bleu(clean_summaries_test_sample_v1, output_summaries_test, smoothing_function=cc.method4)\n",
    "print(BLEUscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "liquid crystal display panel and liquid crystal display panel display panel display panel display panel display panel\n",
      "liquid crystal display panel and manufacturing method thereof\n",
      "present disclosure provides liquid crystal display panel liquid crystal display panel includes first substrate second substrate liquid crystal layer liquid crystal layer first substrate second substrate first substrate includes first alignment layer second substrate includes second alignment layer first alignment layer photo alignment layer second substrate rubbing alignment layer\n"
     ]
    }
   ],
   "source": [
    "index = 15\n",
    "print(output_summaries_test[index])\n",
    "print(clean_summaries_test_sample[index])\n",
    "print(clean_text_test_sample[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_summaries_top10_v1 = []\n",
    "for i in range(0, len(clean_summaries_top10)):\n",
    "    clean_summaries_top10_v1.append([clean_summaries_top10[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1974889828883346\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import SmoothingFunction, sentence_bleu\n",
    "cc = SmoothingFunction()\n",
    "BLEUscore = nltk.translate.bleu_score.sentence_bleu(clean_summaries_top10_v1, output_summaries, smoothing_function=cc.method4)\n",
    "print(BLEUscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2749932070188259\n"
     ]
    }
   ],
   "source": [
    "BLEUscore = nltk.translate.bleu_score.sentence_bleu(clean_summaries_test_sample_v1[3], output_summaries_test[3])\n",
    "print(BLEUscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mobile unit having internet protocol functionality']"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_summaries_test_sample_v1[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mobile unit with a wireless communication connection multimedia device'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_summaries_test[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_nlp_proj2",
   "language": "python",
   "name": "env_nlp2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
